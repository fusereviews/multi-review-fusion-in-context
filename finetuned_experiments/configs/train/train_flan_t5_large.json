{
    // Model config
    "experiment_type": "seq2seq",
    "model_name_or_path": "google/flan-t5-large",
    "source_prefix": "",
    "max_source_length": 1500,
    "max_target_length": 200,
    "output_dir": "models/flan-t5-large/flan_t5_large",
    "overwrite_cache": true,
    "include_inputs_for_metrics": true,
    "min_length": 100,
    "length_penalty": 2.0,
    "early_stopping": true,
    "no_repeat_ngram_size": 3,
    "fp16": false, // Recommended by docs
    // Predict
    "predict_with_generate": true,  
    "num_beams": 2,  // Lever to play with if getting OOM, was suggested in docs
    // Train              
    "do_train": true,
    "per_device_train_batch_size": 1,
    "overwrite_output_dir": "true",
    "train_file": "data/train__highlights_with_full_inputs.csv",
    "save_total_limit": 2,  // Save only last one and best one
    "metric_for_best_model": "eval_gold_rougeL",
    "load_best_model_at_end": true,
    "max_grad_norm": 1.0,
    // Eval while training
    "do_eval": true,
    "per_device_eval_batch_size": 2,  // Can be larger than training batch size (no grad is activated)
    "validation_file": "data/dev__highlights_with_full_inputs.csv",
    "evaluation_strategy": "steps",
    "save_steps": 100,
    "eval_steps": 100,
    "logging_steps": 100,
    "num_train_epochs": 10.0,
    // Wandb
    "report_to": "wandb",
    "run_name": "flan-t5-large/flan_t5_large"
    }